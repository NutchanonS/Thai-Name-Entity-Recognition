{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dey-gc5QSGsE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvLvuTfKSHs1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0GNh6nVSTJX"
      },
      "outputs": [],
      "source": [
        "root = f'/content/drive/MyDrive/Colab Notebooks/SuperAI2_Hackathon4/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDVtqJv3T9B8"
      },
      "outputs": [],
      "source": [
        "path_ne_csv = os.path.join(root,'ne_sample_submission.csv')\n",
        "path_ne_txt = os.path.join(root,'ne_test.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_2ktd8haBGJ"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/SuperAI2_Hackathon4/ne_test.txt') as f:\n",
        "    contents = f.read()\n",
        "words = contents.split(\"\\n\")\n",
        "wordss = words[:69561]   #deleted 2 last spaces\n",
        "\n",
        "wordsss = np.array(wordss)\n",
        "word_list = list(wordsss[0:])\n",
        "\n",
        "index = 0\n",
        "for word in word_list :\n",
        "    if word == '' : word_list[index] = ' '\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOkriLIdTY_9"
      },
      "source": [
        "# NER for Thai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sWIeGHKZF0_"
      },
      "source": [
        "## load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlchPmMhgk4r"
      },
      "source": [
        "### import pre-trian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCjmut4QimrS"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch==1.5.0 torchtext==0.4.0 torchvision==0.6.0\n",
        "!pip -q install transformers==3.5.0 thai2transformers==0.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d2F_RuyTZ4V"
      },
      "outputs": [],
      "source": [
        "! pip install -q datasets transformers[sentencepiece] simpletransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMcqcGKaTeYl"
      },
      "outputs": [],
      "source": [
        "# ! wget https://github.com/kobkrit/datasets/raw/main/AIFORTHAI-LST20Corpus.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeyhLAgfTh3d"
      },
      "outputs": [],
      "source": [
        "# ! tar -xvzf AIFORTHAI-LST20Corpus.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeYepC9XTvbn"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "lst20 = load_dataset(\"lst20\", data_dir=\"/content/drive/MyDrive/LST20_Corpus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKhRUpHWTydh"
      },
      "outputs": [],
      "source": [
        "lst20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWyQlTsZMeO"
      },
      "source": [
        "## import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xhMHVg8T1iD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(lst20['train'])\n",
        "validation_df = pd.DataFrame(lst20['validation'])\n",
        "test_df = pd.DataFrame(lst20['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBxvhx9qURUu"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMgd3Dj_UDFw"
      },
      "outputs": [],
      "source": [
        "validation_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3A2zyE-UMuA"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZpwWdUjUVlP"
      },
      "outputs": [],
      "source": [
        "# See tags from https://github.com/huggingface/datasets/blob/master/datasets/lst20/lst20.py\n",
        "_POS_TAGS = [\"NN\", \"VV\", \"PU\", \"CC\", \"PS\", \"AX\", \"AV\", \"FX\", \"NU\", \"AJ\", \"CL\", \"PR\", \"NG\", \"PA\", \"XX\", \"IJ\"]\n",
        "_NER_TAGS = [\n",
        "        \"O\",\n",
        "        \"B_BRN\",\n",
        "        \"B_DES\",\n",
        "        \"B_DTM\",\n",
        "        \"B_LOC\",\n",
        "        \"B_MEA\",\n",
        "        \"B_NUM\",\n",
        "        \"B_ORG\",\n",
        "        \"B_PER\",\n",
        "        \"B_TRM\",\n",
        "        \"B_TTL\",\n",
        "        \"I_BRN\",\n",
        "        \"I_DES\",\n",
        "        \"I_DTM\",\n",
        "        \"I_LOC\",\n",
        "        \"I_MEA\",\n",
        "        \"I_NUM\",\n",
        "        \"I_ORG\",\n",
        "        \"I_PER\",\n",
        "        \"I_TRM\",\n",
        "        \"I_TTL\",\n",
        "        \"E_BRN\",\n",
        "        \"E_DES\",\n",
        "        \"E_DTM\",\n",
        "        \"E_LOC\",\n",
        "        \"E_MEA\",\n",
        "        \"E_NUM\",\n",
        "        \"E_ORG\",\n",
        "        \"E_PER\",\n",
        "        \"E_TRM\",\n",
        "        \"E_TTL\",\n",
        "    ]\n",
        "_CLAUSE_TAGS = [\"O\", \"B_CLS\", \"I_CLS\", \"E_CLS\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn4cQsAbUXYl"
      },
      "outputs": [],
      "source": [
        "list(map(lambda x: _NER_TAGS[x], train_df[\"ner_tags\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmTdig4JZUDl"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPP58SGgUiDo"
      },
      "outputs": [],
      "source": [
        "def convert_to_simple_transformer_format(df, field_name, tags):\n",
        "  sentence_id = []\n",
        "  words = []\n",
        "  labels = []\n",
        "\n",
        "  #Limit at 1000 rows for speed.\n",
        "  for (idx, r) in df[:1000].iterrows():\n",
        "    # print(idx)\n",
        "    for (i, t) in enumerate(r['tokens']):\n",
        "      # print(i,t)\n",
        "      sentence_id.append(idx)\n",
        "      words.append(t)\n",
        "      labels.append(tags[r[field_name][i]])\n",
        "\n",
        "  return pd.DataFrame(\n",
        "      {\"sentence_id\": sentence_id, \"words\": words, \"labels\": labels}\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq6sOMhTW06s"
      },
      "outputs": [],
      "source": [
        "train_ = convert_to_simple_transformer_format(train_df, \"ner_tags\", _NER_TAGS)\n",
        "train_.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFkMeclmXtrP"
      },
      "outputs": [],
      "source": [
        "validation_ = convert_to_simple_transformer_format(validation_df, \"ner_tags\", _NER_TAGS)\n",
        "validation_.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiDP8HNUZhAa"
      },
      "outputs": [],
      "source": [
        "test_ = convert_to_simple_transformer_format(test_df, \"ner_tags\", _NER_TAGS)\n",
        "test_.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTrbvo7NkDkn"
      },
      "source": [
        "## padded_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06c6tlarro29"
      },
      "outputs": [],
      "source": [
        "train_[['words','labels'][:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYY5SoaJkmzV"
      },
      "outputs": [],
      "source": [
        "train_list = []\n",
        "for d in zip(train_['words'].iteritems(), train_['labels'].iteritems()):\n",
        "  train_list.append([d[0][1], d[1][1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqFbsem7ue6X"
      },
      "outputs": [],
      "source": [
        "train_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeF7T7sWs9Fo"
      },
      "outputs": [],
      "source": [
        "# def map_record_to_training_data(record):\n",
        "#   print(record)\n",
        "\n",
        "# def lowercase_and_convert_to_ids(tokens):\n",
        "#   tokens = tf.strings.lower(tokens)\n",
        "#   return lookup_layer(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zC43Qd4yCaS"
      },
      "outputs": [],
      "source": [
        "train_list = tf.data.Dataset.from_tensor_slices(train_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U2LZIWykM18"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset = (\n",
        "    train_list\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "\n",
        "# val_dataset = (\n",
        "#     validation_.map(map_record_to_training_data)\n",
        "#     .padded_batch(batch_size)\n",
        "# )\n",
        "\n",
        "# test_dataset = (\n",
        "#     test_.map(map_record_to_training_data)\n",
        "#     .padded_batch(batch_size)\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYfvRnCJZZre"
      },
      "source": [
        "## build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4ViDKfCLqZ6"
      },
      "source": [
        "## airesearch/wangchanberta-base-att-spm-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Zwfi9EHY_VX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from simpletransformers.ner import NERModel, NERArgs\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "#transformers\n",
        "from transformers import (\n",
        "    CamembertTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "\n",
        "#thai2transformers\n",
        "import thai2transformers\n",
        "from thai2transformers.preprocess import process_transformers\n",
        "from thai2transformers.metrics import (\n",
        "    classification_metrics, \n",
        "    multilabel_classification_metrics,\n",
        ")\n",
        "from thai2transformers.tokenizers import (\n",
        "    ThaiRobertaTokenizer,\n",
        "    ThaiWordsNewmmTokenizer,\n",
        "    ThaiWordsSyllableTokenizer,\n",
        "    FakeSefrCutTokenizer,\n",
        "    SEFR_SPLIT_TOKEN\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Configure the model\n",
        "ner_args = NERArgs()\n",
        "ner_args.train_batch_size = 16\n",
        "ner_args.evaluate_during_training = True\n",
        "ner_args.overwrite_output_dir = True\n",
        "ner_args.num_train_epochs = 5\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
        "\n",
        "model = NERModel(\n",
        "    \"camembert\", \"airesearch/wangchanberta-base-att-spm-uncased\", \n",
        "    # \"bert\", \"monsoon-nlp/bert-base-thai\", \n",
        "    args = ner_args, \n",
        "    use_cuda = torch.cuda.is_available(), \n",
        "    labels = _NER_TAGS\n",
        "    # model_args.lazy_loading = True\n",
        "\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.train_model(train_, eval_data=validation_, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvDza6u0aeUq"
      },
      "source": [
        "## eval test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0nzg5TEyGZh"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/Colab Notebooks/Model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NnaN6KnZd0h"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "result, model_outputs, preds_list = model.eval_model(test_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywz9H8iUgwt_"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Pzhr9rgxAd"
      },
      "outputs": [],
      "source": [
        "test__ = list(test_['words'])\n",
        "test__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5317YwqohFRQ"
      },
      "outputs": [],
      "source": [
        "predictions, raw_outputs = model.predict(test__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WarCy6ViUnNi"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pySh4FUOkH9S"
      },
      "outputs": [],
      "source": [
        "for i, s in enumerate(predictions):\n",
        "  print(i,s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACJf-4zaq-4N"
      },
      "outputs": [],
      "source": [
        "result,predictions, raw_outputs = model.predict(word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXP0t1VhAh99"
      },
      "source": [
        "## BI-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-J54FSNB1s_"
      },
      "outputs": [],
      "source": [
        "raw_outputs_df=pd.DataFrame(raw_outputs)\n",
        "print(raw_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P9XbkyNA1qx"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmMBd7gD0Pv"
      },
      "source": [
        "# submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPUePNFKEd9A"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "for i, p in enumerate(predictions):\n",
        "  if len(p) > 0:\n",
        "    for k, v in p[0].items():\n",
        "      j = v\n",
        "  else:\n",
        "    j = 'O'\n",
        "\n",
        "  labels.append([i+1,j])\n",
        "print(len(labels) , labels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paaVDDqqIAoh"
      },
      "outputs": [],
      "source": [
        "sub = pd.DataFrame(labels, columns=['Id', 'Predicted'])\n",
        "sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNISUg8gKHf9"
      },
      "outputs": [],
      "source": [
        "sub.to_csv('submission.csv', columns=None, header=True, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzIXoKHYR6L_"
      },
      "outputs": [],
      "source": [
        "csv_sub = pd.read_csv('/content/submission.csv')\n",
        "csv_sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4Am5X8bSiiW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dYfvRnCJZZre"
      ],
      "machine_shape": "hm",
      "name": "Mini2_NER_Bert_BY_BEUK.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
