{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHrlITUWV95S"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC-ZgjhbWIg5"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH3IWnAlrsHL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j1UttloWF_B"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnmR7B9nWPKZ"
      },
      "source": [
        "## data Lst20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VGNBBo3f4FQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "path = '/content/train2'\n",
        "wordss = []\n",
        "poss = []\n",
        "counter = 0\n",
        "word = []\n",
        "pos = []\n",
        "for dir in sorted(os.listdir(path)):\n",
        "    if dir != \".DS_Store\" and dir.startswith('T'):\n",
        "      #print(dir)\n",
        "      f = open(os.path.join(path,dir), \"r\")\n",
        "\n",
        "      #print(f.read())\n",
        "\n",
        "\n",
        "      for i in f :\n",
        "        #print(i.split())\n",
        "        if len(i.split()) == 0 :\n",
        "          \n",
        "          wordss.append(word)\n",
        "          poss.append(pos)\n",
        "          word = []\n",
        "          pos = []\n",
        "          # print(wordss)\n",
        "          # print(poss)\n",
        "          continue\n",
        "        word.append(i.split(\"\\t\")[0])\n",
        "        pos.append(i.split(\"\\t\")[2])\n",
        "        # if counter > 24:\n",
        "        #   break\n",
        "        # counter +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hdekD1F_32L"
      },
      "outputs": [],
      "source": [
        "wordss = list(filter(lambda x: len(x) <= 150, wordss))\n",
        "poss=list(filter(lambda x: len(x) <= 150, poss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTGat3j83Q71"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "wordss = list(filter(lambda x: len(x) <= 150, wordss))\n",
        "poss = list(filter(lambda x: len(x) <= 150, poss))\n",
        "tags=['B_BRN', 'B_DES', 'B_DTM', 'B_LOC', 'B_MEA', 'B_NUM', 'B_ORG', 'B_PER', \n",
        "      'B_TRM', 'B_TTL', 'E_BRN', 'E_DES', 'E_DTM', 'E_LOC', 'E_MEA', 'E_NUM', \n",
        "      'E_ORG', 'E_PER', 'E_TRM', 'E_TTL', 'I_BRN', 'I_DES', 'I_DTM', 'I_LOC', \n",
        "      'I_MEA', 'I_NUM', 'I_ORG', 'I_PER', 'I_TRM', 'I_TTL', 'O']\n",
        "for ts in range(len(poss)):\n",
        "  for tag in range(len(poss[ts])):\n",
        "    if poss[ts][tag]=='ORG_I':\n",
        "      poss[ts][tag]='I_ORG'\n",
        "\n",
        "    elif poss[ts][tag]=='LOC_I':\n",
        "      poss[ts][tag]='I_LOC'\n",
        "\n",
        "    elif poss[ts][tag]=='PER_I':\n",
        "      poss[ts][tag]='I_PER'\n",
        "    elif poss[ts][tag] not in tags:\n",
        "      poss[ts][tag]='O'\n",
        "  \n",
        "for s in range(len(wordss)) :\n",
        "    for w in range(len(wordss[s])):\n",
        "        try:\n",
        "          a=float(wordss[s][w]) +1\n",
        "          wordss[s][w]='NUM'\n",
        "        except:\n",
        "          pass\n",
        "(train_sentences, test_sentences, train_tags, test_tags) = train_test_split(wordss, poss, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBrLDvZpiQzt"
      },
      "outputs": [],
      "source": [
        "word2index={}\n",
        "word2index_save=[]\n",
        "total_word=set()\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['PAD'] = 0  \n",
        "\n",
        "for i in wordss:\n",
        "  for j in i:\n",
        "    total_word.add(j)\n",
        "\n",
        "\n",
        "word2index = { w :i + 2 for i, w in enumerate(wordss)}\n",
        "word2index['PAD'] = 0  \n",
        "word2index['UNK'] = 1   \n",
        "\n",
        "# word2index_save = list((w ,i + 2 ) for i, w in enumerate(wordss))\n",
        "# word2index_save.append(('PAD',0))\n",
        "# word2index_save.append(('UNK',1))\n",
        "# df=pd.DataFrame(word2index_save,columns=['words','indexs'])\n",
        "# df.to_csv('/content/my_dict.csv')\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV3vCpwuK4uR"
      },
      "source": [
        "### Save dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3E6m7FkouJE"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(word2index, open('/content/dict_word.p', 'wb'), protocol =4)\n",
        "# # with open('/content/drive/MyDrive/SUPERAI2-452 /NLP#1/cat_train_tags_y_3.p', \"rb\") as fh:\n",
        "# #   cat_train_tags_y = pickle.load(fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_zbLTLm_DPo"
      },
      "outputs": [],
      "source": [
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w]) #.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['UNK'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        " \n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w]) \n",
        "        except KeyError:\n",
        "            s_int.append(word2index['UNK'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in train_tags:\n",
        "    a=[]\n",
        "    for t in s:\n",
        "        try:\n",
        "            a.append(tag2index[t])\n",
        "        except:\n",
        "            a.append(tag2index['O'])\n",
        "    train_tags_y.append(a)\n",
        "for s in test_tags:\n",
        "    a=[]\n",
        "    for t in s:\n",
        "        try:\n",
        "            a.append(tag2index[t])\n",
        "        except:\n",
        "            a.append(tag2index['O'])\n",
        "    test_tags_y.append(a)\n",
        "    \n",
        "#print(train_sentences_X[0])\n",
        "#print(test_sentences_X[0])\n",
        "#print(train_tags_y[0])\n",
        "#print(test_tags_y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qwt5q7YWdBx"
      },
      "source": [
        "## Data Lst21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGZkYh7qv9JS",
        "outputId": "7f1364ae-24c0-4f9d-8cec-894305d19d92"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "path_lst21_1 = '/content/LST21/LST21/lst21_1/'\n",
        "word21=[[]]\n",
        "tag21=[[]]\n",
        "for file in sorted(os.listdir(path_lst21_1)):\n",
        "  text = pd.read_csv(path_lst21_1+file , sep=' /', header=None, skip_blank_lines=False)\n",
        "  text=text.fillna(' ')\n",
        "  for sent in text[0]:\n",
        "    if sent!=' ':\n",
        "      word21[len(word21)-1].append(sent.split('\\t')[0])\n",
        "      tag21[len(tag21)-1].append(sent.split('\\t')[2])\n",
        "    else:\n",
        "      word21.append([])\n",
        "      tag21.append([])\n",
        "del word21[-1]\n",
        "del tag21[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gUl_M0ZJDph"
      },
      "outputs": [],
      "source": [
        "a=0\n",
        "for i in word21:\n",
        "  if len(i)>150:\n",
        "    a +=1\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2BTSnJuImYh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "word21 = list(filter(lambda x: len(x) <= 150, word21))\n",
        "tag21 = list(filter(lambda x: len(x) <= 150, tag21))\n",
        "tags=['B_BRN', 'B_DES', 'B_DTM', 'B_LOC', 'B_MEA', 'B_NUM', 'B_ORG', 'B_PER', \n",
        "      'B_TRM', 'B_TTL', 'E_BRN', 'E_DES', 'E_DTM', 'E_LOC', 'E_MEA', 'E_NUM', \n",
        "      'E_ORG', 'E_PER', 'E_TRM', 'E_TTL', 'I_BRN', 'I_DES', 'I_DTM', 'I_LOC', \n",
        "      'I_MEA', 'I_NUM', 'I_ORG', 'I_PER', 'I_TRM', 'I_TTL', 'O']\n",
        "for ts in range(len(tag21)):\n",
        "  for tag in range(len(tag21[ts])):\n",
        "    if tag21[ts][tag]=='ORG_I':\n",
        "      tag21[ts][tag]='I_ORG'\n",
        "\n",
        "    elif tag21[ts][tag]=='LOC_I':\n",
        "      tag21[ts][tag]='I_LOC'\n",
        "\n",
        "    elif tag21[ts][tag]=='PER_I':\n",
        "      tag21[ts][tag]='I_PER'\n",
        "    elif tag21[ts][tag] not in tags:\n",
        "      tag21[ts][tag]='O'\n",
        "  \n",
        "for s in range(len(word21)) :\n",
        "    for w in range(len(word21[s])):\n",
        "        try:\n",
        "          a=float(word21[s][w]) +1\n",
        "          word21[s][w]='NUM'\n",
        "        except:\n",
        "          pass\n",
        "(train_sentences, test_sentences, train_tags, test_tags) = train_test_split(word21, tag21, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PjaNnEDKxTJ"
      },
      "source": [
        "### Load dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoA4_iYoIyhQ"
      },
      "outputs": [],
      "source": [
        "# with open('/content/dict_word.p', \"rb\") as fh:\n",
        "#   word2index = pickle.load(fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUrWqPlBigv7"
      },
      "outputs": [],
      "source": [
        "sample_submission=[]\n",
        "for i,sent in enumerate(sentence_list):\n",
        "  for j, word in enumerate(sent):\n",
        "    sample_submission.append(submis[i][j])\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iry7ayYpJ3bS"
      },
      "outputs": [],
      "source": [
        "word2index={}\n",
        "word2index_save=[]\n",
        "total_word=set()\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['PAD'] = 0  \n",
        "\n",
        "for i in word21:\n",
        "  for j in i:\n",
        "    total_word.add(j)\n",
        "\n",
        "\n",
        "word2index = { w :i + 2 for i, w in enumerate(list(total_word))}\n",
        "word2index['PAD'] = 0  \n",
        "word2index['UNK'] = 1   \n",
        "\n",
        "# pickle.dump(word2index, open('/content/dict_word.p', 'wb'), protocol =4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUo1087gKTwE"
      },
      "outputs": [],
      "source": [
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w]) #.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['UNK'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w]) \n",
        "        except KeyError:\n",
        "            s_int.append(word2index['UNK'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        "for s in train_tags:\n",
        "    a=[]\n",
        "    for t in s:\n",
        "        try:\n",
        "            a.append(tag2index[t])\n",
        "        except:\n",
        "            a.append(tag2index['O'])\n",
        "    train_tags_y.append(a)\n",
        "for s in test_tags:\n",
        "    a=[]\n",
        "    for t in s:\n",
        "        try:\n",
        "            a.append(tag2index[t])\n",
        "        except:\n",
        "            a.append(tag2index['O'])\n",
        "    test_tags_y.append(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsNngxhvy4qs"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZKOv47ZazqO"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# model = tf.keras.models.load_model('../input/model-lst20/model_NER_Pgot.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEuQ0eml_J_2",
        "outputId": "ce0df55e-7280-4e0e-a03f-32a6d1c625b1"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)  # 271"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlLAQjMR_Slp"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        " \n",
        "#print(train_sentences_X[0])\n",
        "#print(test_sentences_X[0])\n",
        "#print(train_tags_y[0])\n",
        "#print(test_tags_y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7cklfO08BNG",
        "outputId": "db2fd173-c8e0-4521-c407-694d8a913b78"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt2IORJ3_Uu8",
        "outputId": "3ebbbc18-c492-47cb-f5e9-c11570f4a598"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 100))\n",
        "# model.add(keras.layers.Conv1D(1, (4,4), activation='relu',strides=4))\n",
        "# model.add(keras.layers.MaxPool1D(pool_size=2, strides=2)) \n",
        "# model.add(keras.layers.Conv1D(1, (4,4), activation='relu',strides=4))\n",
        "# model.add(keras.layers.MaxPool1D(pool_size=2, strides=2)) \n",
        "# model.add(Flatten())\n",
        "\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrXHaoUj_qmr"
      },
      "outputs": [],
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL0abdIOI91W",
        "outputId": "f123a082-6b71-4541-bbcc-2a7cc2914696"
      },
      "outputs": [],
      "source": [
        "train_tags_y, train_tags_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b70qa_ObDDS5",
        "outputId": "7c9d595d-5e41-41d1-9745-25fc20394c32"
      },
      "outputs": [],
      "source": [
        "train_tags_y[0]\n",
        "print(len(tag2index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49zqS22O_usQ",
        "outputId": "ce14696d-8474-4517-8f38-3a8871d495d9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "print(cat_train_tags_y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrtc8Vkg_xWB",
        "outputId": "318238d5-9a42-479f-a4e8-e6dba6dea749"
      },
      "outputs": [],
      "source": [
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=32, epochs=5, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AhKmbha_2DK"
      },
      "outputs": [],
      "source": [
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "093X_1FMe6rN"
      },
      "outputs": [],
      "source": [
        "model.save('/content/model_NER_Pgot.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvIpeLNZWw1y"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJuTWdARXFaE"
      },
      "outputs": [],
      "source": [
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        "            \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riH9Ym2VW1SA"
      },
      "source": [
        "## try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGoVesEdHK_k"
      },
      "outputs": [],
      "source": [
        "a='และ ยืนยัน ว่า ประชาชน ใน ภาค ใต้ ไม่ มีการ ซื้อ เสียง'\n",
        "test_samples=[a.split()]\n",
        "print(test_samples)# = [['นาง','สุดารัตน์','เกยุราพันธ์','รมว.','สาธารณสุข']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmxMs53tATCK"
      },
      "outputs": [],
      "source": [
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    print(s)\n",
        "    s_int = []\n",
        "    \n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w]) #.lower()])\n",
        "            print(s_int)\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-UNK-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNo4bSfsAVdv"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZpK0-DwAeVh"
      },
      "outputs": [],
      "source": [
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePZR9u-QXISg"
      },
      "source": [
        "## Submission1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxqQgqqkfZ1z"
      },
      "outputs": [],
      "source": [
        "#https://drive.google.com/file/d/17tcUt5uTkCS2hx5i7lxhKnQpBBijCc5G/view?usp=sharing\n",
        "!gdown --id 17tcUt5uTkCS2hx5i7lxhKnQpBBijCc5G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n_BRwjWLBsl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "path_test = '/content/ne_test.txt'\n",
        "ne_test_pd = pd.read_csv(path_test, sep=' /', header=None, skip_blank_lines=False)\n",
        "ne_test_pd=ne_test_pd.fillna('_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaIAXCZAjKmb"
      },
      "outputs": [],
      "source": [
        "check_predict=[]\n",
        "test_samples_X=[]\n",
        "for s in ne_test_pd[0]:\n",
        "    try:\n",
        "        try:\n",
        "          a=float(s)+1\n",
        "          test_samples_X.append('NUM')\n",
        "        except:\n",
        "          test_samples_X.append(word2index[s]) #.lower()])\n",
        "          check_predict.append(s)\n",
        "    except KeyError:\n",
        "        test_samples_X.append(word2index['UNK'])\n",
        "        check_predict.append('UNK')\n",
        "\n",
        "nu_test_sent = []\n",
        "check_pred=[]\n",
        "for i in range(0,len(test_samples_X),MAX_LENGTH):\n",
        "  nu_test_sent.append(test_samples_X[i:i+MAX_LENGTH])\n",
        "  check_pred.append(check_predict[i:i+MAX_LENGTH])\n",
        "\n",
        "a=len(nu_test_sent[-1]) \n",
        "b=len(nu_test_sent)\n",
        "for i in range(MAX_LENGTH-a):\n",
        "  nu_test_sent[-1].append(word2index['PAD']) # PAD last index\n",
        "  check_pred[-1].append('PAD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJeqF_9JpmiL"
      },
      "outputs": [],
      "source": [
        "nu_test_sent=np.array(nu_test_sent)\n",
        "# nu_test_sent=tf.data.Dataset.from_tensor_slices(nu_test_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3yEyk00kKIy"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(nu_test_sent)\n",
        "print(predictions, predictions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4ZFuKMOssBc"
      },
      "outputs": [],
      "source": [
        "submis=logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoFXekU7zFZi"
      },
      "outputs": [],
      "source": [
        "sample_submission=[]\n",
        "for i in submis:\n",
        "  for j in i:\n",
        "    # if j=='-PAD-': break\n",
        "    sample_submission.append(j)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-X7Ri1v6Dki"
      },
      "outputs": [],
      "source": [
        "len(sample_submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjpyMF_76IQS"
      },
      "outputs": [],
      "source": [
        "len(test_samples_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJsDZbri1oKk"
      },
      "outputs": [],
      "source": [
        "sub  =  sample_submission[0:len(test_samples_X)-1]\n",
        "print(len(sub))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bKVHQzv3GUL"
      },
      "outputs": [],
      "source": [
        "num_csv=np.arange(69562).tolist()\n",
        "print(num_csv[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7ShwY813nGq"
      },
      "outputs": [],
      "source": [
        "ne_test_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC6GuJLj7LPx"
      },
      "outputs": [],
      "source": [
        "sub_csv = []\n",
        "for n, i in enumerate(sub):\n",
        "  sub_csv.append([n+1, i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRS_6bHC7MqS"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(sub_csv, columns=['Id','Predicted']).to_csv(f'submission.csv', index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdmeZ45i3aZV"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame([num_csv,sub],columns=['Id','Predicted'],index=False)\n",
        "df.to_csv('/content/sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgbBLMcXv5az"
      },
      "outputs": [],
      "source": [
        "print(check_pred[0])\n",
        "print(submis[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Task3_By_Pgot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
